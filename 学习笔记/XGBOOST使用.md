# XGBOOST使用

## 导入

`import xgboost as xgb`

- 示例

```python
import pandas as pd
import numpy as np
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn import cross_validation, metrics   #Additional scklearn functions
from sklearn.grid_search import GridSearchCV   #Perforing grid search
```
**XGBClassifier** – 这是 XGBoost 的 sklearn 包装器。这允许我们以与 GBM 相同的方式使用 sklearn 的 Grid Search 进行并行处理，同样有**xgboost.** **XGBRegressor**

## 数据转换

`xgb.DMatrix`

- 要将 NumPy 数组加载到[`DMatrix`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix)：
  ```python
  data = np.random.rand(5, 10)  # 5 entities, each contains 10 features
  label = np.random.randint(2, size=5)  # binary target
  dtrain = xgb.DMatrix(data, label=label)
  ```
- 要将 Pandas 数据框加载到[`DMatrix`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix)：
  ```python
  data = pandas.DataFrame(np.arange(12).reshape((4,3)), columns=['a', 'b', 'c'])
  label = pandas.DataFrame(np.random.randint(2, size=4))
  dtrain = xgb.DMatrix(data, label=label)
  ```
- 缺失值可以替换为[`DMatrix`](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix)构造函数中的默认值：
  ```python
  dtrain = xgb.DMatrix(data, label=label, missing=-999.0)
  ```

- 可以在需要时设置权重：
  ```python
  w = np.random.rand(5, 1)
  dtrain = xgb.DMatrix(data, label=label, missing=-999.0, weight=w)
  ```

## 学习

###`xgboost.train(params,dtrain, ……)`
**params** (dict) – Booster params.
**dtrain** (DMatrix) – Data to be trained.
**params:**

- **booster :** gbtree, 用什么方法进行结点分裂。梯度提升树来进行结点分裂。
- **objective** : multi softmax, 使用的损失函数，softmax 是多分类问题
- **num_class** : 10, 类别数，与 multi softmax 并用
- **gamma** : 损失下降多少才进行分裂
- **max_depth** : 12, 构建树的深度, 越大越容易过拟合
- **lambda** : 2, 控制模型复杂度的权重值的L2正则化项参数，参数越大。模型越不容易过拟合。
- **subsample** : 0.7 , 随机采样训练样本，取70%的数据训练
- **colsample_bytree** : 0.7, 生成树时进行的列采样
- **min_child_weight** : 3, 孩子节点中最小的样本权重和，如果一个叶子结点的样本权重和小于 min_child_weight 则拆分过程结果
- **slient** : 0, 设置成 1 则没有运行信息输出，最好是设置为0
- **eta** : 0.007, 如同学习率。前面的树都不变了，新加入一棵树后对结果的影响占比
- **seed** : 1000
- **Thread** : 7, cup 线程数
```python
# simple example
# load file from text file, also binary buffer generated by xgboost
dtrain = xgb.DMatrix(os.path.join(DEMO_DIR, 'data', 'agaricus.txt.train?indexing_mode=1'))
dtest = xgb.DMatrix(os.path.join(DEMO_DIR, 'data', 'agaricus.txt.test?indexing_mode=1'))

# specify parameters via map, definition are same as c++ version
param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}

# specify validations set to watch performance
watchlist = [(dtest, 'eval'), (dtrain, 'train')]
num_round = 2
bst = xgb.train(param, dtrain, num_round, watchlist)

# this is prediction
preds = bst.predict(dtest)
labels = dtest.get_label()
print('error=%f' %
      (sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i]) /
       float(len(preds))))
bst.save_model('0001.model')
# dump model
bst.dump_model('dump.raw.txt')
# dump model with feature map
bst.dump_model('dump.nice.txt', os.path.join(DEMO_DIR, 'data/featmap.txt'))

# save dmatrix into binary buffer
dtest.save_binary('dtest.buffer')
# save model
bst.save_model('xgb.model')
# load model and data in
bst2 = xgb.Booster(model_file='xgb.model')
dtest2 = xgb.DMatrix('dtest.buffer')
preds2 = bst2.predict(dtest2)
# assert they are the same
assert np.sum(np.abs(preds2 - preds)) == 0

# alternatively, you can pickle the booster
pks = pickle.dumps(bst2)
# load model and data in
bst3 = pickle.loads(pks)
preds3 = bst3.predict(dtest2)
# assert they are the same
assert np.sum(np.abs(preds3 - preds)) == 0
```

##交叉验证
`xgboost.cv(params,dtrain,num_boost_round=10,nfold=3,metrics=( ),)`

## 回调
XGBoost 有许多预定义的回调，用于支持提前停止、检查点等
```python
D_train = xgb.DMatrix(X_train, y_train)
D_valid = xgb.DMatrix(X_valid, y_valid)

# Define a custom evaluation metric used for early stopping.
def eval_error_metric(predt, dtrain: xgb.DMatrix):
    label = dtrain.get_label()
    r = np.zeros(predt.shape)
    gt = predt > 0.5
    r[gt] = 1 - label[gt]
    le = predt <= 0.5
    r[le] = label[le]
    return 'CustomErr', np.sum(r)

# Specify which dataset and which metric should be used for early stopping.
early_stop = xgb.callback.EarlyStopping(rounds=early_stopping_rounds,
                                        metric_name='CustomErr',
                                        data_name='Train')

booster = xgb.train(
    {'objective': 'binary:logistic',
     'eval_metric': ['error', 'rmse'],
     'tree_method': 'hist'}, D_train,
    evals=[(D_train, 'Train'), (D_valid, 'Valid')],
    feval=eval_error_metric,
    num_boost_round=1000,
    callbacks=[early_stop],
    verbose_eval=False)
```

- **evals** (列表([DMatrix](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix) *,* string )) – 训练期间将评估其指标的验证集列表。验证指标将帮助我们跟踪模型的性能。

- **obj** ( *function* ) – 自定义目标函数。

- **feval** ( *function* ) – 自定义评估函数。

- **early_stopping_rounds** ( [*int*](https://docs.python.org/3.6/library/functions.html#int) ) – 激活提前停止。验证指标需要在每个**early_stopping_rounds**轮中至少改进一次才能继续训练。需要至少一项**evals**。该方法返回最后一次迭代（不是最好的）的模型。如果需要最佳模型，请使用自定义回调或模型切片。如果**evals 中**有多个项目，则最后一个条目将用于提前停止。如果**params 中**给出的**eval_metric**参数中 有多个指标，则最后一个指标将用于提前停止。如果发生提前停止，模型将具有三个附加字段：`bst.best_score`, `bst.best_iteration`.

- **callbacks** *(回调函数列表*）–在每次迭代结束时应用的回调函数列表。可以通过使用[Callback API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#callback-api)来使用预定义的回调 。

### 回调API

- **xgboost.callback.** **TrainingCallback** **(** **)**
训练回调接口
- **xgboost.callback.** **EvaluationMonitor** ***( rank=0, period=1,show_stdv=False)***
在每次迭代时打印评估结果
- **xgboost.callback.** **EarlyStopping** ***(rounds, metric_name=None,data_name=None,maximize=None,save_best=False,min_delta=0.0)***
提前停止的回调函数
-**xgboost.callback.** **LearningRateScheduler** ***(learning_rates)***
用于调度学习率的回调函数。

都没有**return**

## sklearn应用

用法和sklearn包一致

参数：

- **n_estimators** ( [*int*](https://docs.python.org/3.6/library/functions.html#int) ) – 提升轮数。

- **use_label_encoder** ( [*bool*](https://docs.python.org/3.6/library/functions.html#bool) ) –（已弃用）使用 scikit-learn 中的标签编码器对标签进行编码。对于新代码，我们建议您将此参数设置为 False。

- **max_depth** ( *Optional* *[* [*int*](https://docs.python.org/3.6/library/functions.html#int) *]* ) – 基础学习器的最大树深度。

- **learning_rate** ( *Optional* *[* [*float*](https://docs.python.org/3.6/library/functions.html#float) *]* ) – 提高学习率（xgb 的“eta”）

- **verbosity** ( *Optional* *[* [*int*](https://docs.python.org/3.6/library/functions.html#int) *]* ) – 详细程度。有效值为 0（静默）- 3（调试）。

- **object**（*typing.Union* *[* [*str*](https://docs.python.org/3.6/library/stdtypes.html#str) *,* *typing.Callable* *[* *[* [*numpy.ndarray*](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) *,* [*numpy.ndarray*](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) *]* *,* *Typing.Tuple* *[* [*numpy.ndarray*](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) *,* [*numpy.ndarray*](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) *]* *]* *,* *NoneType* *]*）–指定学习任务和相应的学习目标或要使用的自定义目标函数（请参阅下面的注释）。

- **n_jobs** (*可选**[* [*int*](https://docs.python.org/3.6/library/functions.html#int) *]* ) – 用于运行 xgboost 的并行线程数。当与网格搜索等其他 Scikit-Learn 算法一起使用时，您可以选择并行化和平衡线程的算法。创建线程争用将显着降低两种算法的速度。

- **gamma** (*可选**[* [*float*](https://docs.python.org/3.6/library/functions.html#float) *]* ) – 在树的叶节点上进行进一步分区所需的最小损失减少。

- **min_child_weight** ( *Optional* *[* [*float*](https://docs.python.org/3.6/library/functions.html#float) *]* ) – 子**节点**所需的最小实例权重（hessian）总和。

- **max_delta_step** ( *Optional* *[* [*float*](https://docs.python.org/3.6/library/functions.html#float) *]* ) – 我们允许每棵树的权重估计的最大增量步长。

- **subsample** ( *Optional* *[* [*float*](https://docs.python.org/3.6/library/functions.html#float) *]* ) – 训练实例的**子样本**比率。

- **colsample_bytree** ( *Optional* *[* [*float*](https://docs.python.org/3.6/library/functions.html#float) *]* ) – 构建每棵树时列的子**样本**比率。

- **colsample_bylevel** ( *Optional* *[* [*float*](https://docs.python.org/3.6/library/functions.html#float) *]* ) – 每个级别的**列子样本**比率。

- **colsample_bynode** ( *Optional* *[* [*float*](https://docs.python.org/3.6/library/functions.html#float) *]* ) – 每个拆分的**列子样本**比率。

- **random_state** (*可选**[* *Union* *[* [*numpy.random.RandomState*](https://numpy.org/doc/stable/reference/random/legacy.html#numpy.random.RandomState) *,* [*int*](https://docs.python.org/3.6/library/functions.html#int) *]* *]* ) 

- 

  随机数种子。

```python
iris = load_iris()
y = iris['target']
X = iris['data']
kf = KFold(n_splits=2, shuffle=True, random_state=rng)
for train_index, test_index in kf.split(X):
    xgb_model = xgb.XGBClassifier(n_jobs=1).fit(X[train_index], y[train_index])
    predictions = xgb_model.predict(X[test_index])
    actuals = y[test_index]
    print(confusion_matrix(actuals, predictions))
```

网格搜索调参
```python
y = boston['target']
X = boston['data']
xgb_model = xgb.XGBRegressor(n_jobs=1)
clf = GridSearchCV(xgb_model,
                   {'max_depth': [2, 4, 6],
                    'n_estimators': [50, 100, 200]}, verbose=1, n_jobs=1)
clf.fit(X, y)
print(clf.best_score_)
print(clf.best_params_)
```
提前停止
```python
X = digits['data']
y = digits['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
clf = xgb.XGBClassifier(n_jobs=1)
clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric="auc",
        eval_set=[(X_test, y_test)])
```

## 调参顺序

1. 选择一组初始参数

2. 改变 max_depth 和 min_child_weight. 

   这些参数对xgboost性能影响最大，因此，他们应该调整第一。

   - max_depth: 树的最大深度。增加这个值会使模型更加复杂，也容易出现过拟合，深度3-10是合理的。 
   - min_child_weight: 正则化参数. 如果树分区中的实例权重小于定义的总和，则停止树构建过程。

3. 调节 gamma 降低模型过拟合风险. 

4. 调节 subsample 和 colsample_bytree 改变数据采样策略. 

5. 调节学习率 eta.

   减小学习率并增大树个数

## 可解释性

###feature_importances

*property* feature_importances_*: numpy.ndarray*

Feature importances property, return depends on importance_type parameter.

- Returns

  **feature_importances_** (array of shape `[n_features]` except for multi-class)linear model, which returns an array with shape (n_features, n_classes)

### SHAP特征图

```python
import xgboost
import shap

# train an XGBoost model
X, y = shap.datasets.boston()
model = xgboost.XGBRegressor().fit(X, y)

# explain the model's predictions using SHAP
# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)
explainer = shap.Explainer(model)
shap_values = explainer(X)

# visualize the first prediction's explanation
shap.plots.waterfall(shap_values[0])
```





